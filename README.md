# LLM-Supported Data Pipeline Construction: An Iterative Approach

_**Abstract:** Large language models (LLMs) have the potential to revolutionize data engineering making it a manageable process even for non-data scientists. Introducing LLMs into data engineering, however, brings new challenges such as a need for constant verification and quality control. To tackle such a process I have designed a methodology for using LLMs to build data pipelines. I demonstrate the methodology by giving the example of a mixed methods research process combined with NLP and GPT-4._

**Links:**

* Flow Chart: [LLMs and Data Pipelines](https://miro.com/app/board/uXjVM5oQRrU=/?share_link_id=552570215729)
* OneNote Project Template: [Project Template](https://1drv.ms/o/s!AsSD_ioRrpmaizbbmHbAn16xLTOR?e=Y2dPks)
* OneNote Project Management Template: [ Project Management Template](https://1drv.ms/o/s!AsSD_ioRrpmaiyVo9dKR6ndVQ26N?e=fOYugo)

## Data Engineering with LLMs 

Since the release of ChatGPT in November 2022, access to a Large Language Model (LLM) has been readily available for the first time. GPT-3.5 (Generative Pre-trained Transformer 3.5), the underlying LLM of ChatGPT can provide impressive support in the translation of ideas, needs, and requirements into functional data pipelines. GPT-3.5 makes knowledge readily available and allows for the immediate application of generated code. As a reasoning engine, GPT-3.5 can advise on how to set up IDEs, software, and Python libraries according to a project’s requirements. This allows even non-data engineers to build complex data pipelines.  

## Increasing Need for Verification and Quality Control 

With the reduced technical challenges, the task of achieving the best possible data flow has become a lot more feasible. This is because time can be allocated to run shorter, more frequent, and focused iterations concerning the desired outputs of the pipeline. However, while data engineering has become significantly easier, building data pipelines with the support of LLMs requires systematic verification, documentation and quality control. Not only for the validity and reliability of the LLMs’ responses but also concerning the generated code and the desired output of the pipeline. This is because LLMs can generate biased or inaccurate results. The liability for the LLMs’ responses, however, lies with the humans who use them. 

## Iterative Methodology for LLM-Supported Data Pipeline Construction 

To streamline the process of constructing a data pipeline with the support of an LLM, I have developed a respective iterative methodology, an example of which can be found in this [flow chart](https://miro.com/app/board/uXjVM5oQRrU=/?share_link_id=552570215729) . The flow chart depicts a process supported by ChatGPT Plus (GPT-4). It also includes two accompanying [OneNote](https://www.onenote.com/?public=1&omkt=en-EN) notebooks which are linked above in the "Links" section. The first notebook is used as a primary project workspace. The second notebook captures thoughts and ideas that arise during the project but are out-of-scope. The latter notebook will be used to develop ideas for multiple uses of the project and its data as well as for documenting best practices. To effectively ask for feedback and peer reviews the method also includes [Miro](https://miro.com) whiteboards and [Loom](https://miro.com) video messages which can be embedded into the Miro board.

## Case Study: Ph.D. Thesis Process Using Mixed Methods, NLP, and GPT 

The flow chart provides a case study of crafting a Ph.D. thesis using mixed methods research, NLP (Natural Language Processing), and GPT. The chart shows how I would approach such a project, given my stack and the technical possibilities as of July 2023. The process can be configured according to each project’s needs and constraints, including restrictions concernings the use of LLMs, data privacy, and security. I want to emphasize that the proposed data pipeline in this project was not built around sensitive data. Respective laws and regulations have to be considered for each project individually.  

## Incorporating Lessons from Recent and Past Project Experiences 

The flow chart is based on experiences from a [recent NLP project](https://github.com/tanwolf/NLP_Requirements-Engineering) where I used ChatGPT Plus (GPT-4) to interpret the outcomes of classical NLP tasks, such as Latent Dirichlet Allocation (LDA) and entity recognition. I found that pairing NLP with GPT-4 comes close to qualitative data analysis and mixed methods research, especially concerning the naming of LDA topics and the interpretation of the combined findings from LDA and entity recognition.   

Mixed methods research ingests and analyses qualitative as well as quantitative data in various formats. To design a flow chart of the research process I contrasted the recent project to my [diploma thesis](quadripolar_identity_model.pdf)  for which I used qualitative data analysis, well before the advent of modern-day data science and NLG.  

Having gone through a qualitative data analysis process without sophisticated software, made me understand that the main challenge is to prevent cognitive overload. Due to the multitude of unstructured data, various data types and formats, it is very challenging to put them into relation to each other and sum up findings. This difficulty increases with the size of the data or datasets.  

## Streamlining LLM-Supported Mixed Methods Research 

As mentioned above incorporating LLMs into the construction of data pipelines requires systematic verification, documentation, and quality control. When adding mixed methods to this process this need only increases. To address this challenge the research process depicted in my flow chart aims to streamline the process of LLM-supported construction of data pipelines. This is achieved by pairing the process with [qualitative data analysis software ](https://www.maxqda.com/) , [referencing software](https://www.citavi.com/en), iterative project management and seamless documentation.  

A primary objective of the depicted project management process is to prevent cognitive overload for the researcher. Cognitive overload can potentially result in either system overload or an unfinished data pipeline or project. To prevent this, a dedicated home is given to every random thought or idea that might arise during the research process. Taking notes frees up the mind and lays a foundation for project documentation. It also makes the process manageable and transparent: Notes can be transformed from Random Thoughts into To Dos, into lists of stakeholders or Relevant Resources. It is key to de-clutter and restructure only those notes that are relevant to the task at hand. In addition, regular rounds of cleaning up are foreseen after every few iterations. While this research process is still a challenging endeavor, it allows for short, focused iterations, and regular quality control.  

Given the current speed of advancements in the field of LLMs, I am expecting the depicted process to become ever more simple over the next few years. This is because more and more software applications are beginning to incorporate LLMs into their software. For example, the [MAXQDA](https://www.maxqda.com/) qualitative data analysis software already has an [AI Assist](https://www.maxqda.com/products/ai-assist) function. A Copilot function for Microsoft OneNote has been announced. Furthermore, as of July 2023, google’s LLM BARD is publicly available in most countries of the world. BARD and ChatGPT in combination can be used to verify the responses of each LLM. 

## Mixed Methods Research, NLP and LLMs 

Qualitative data analysis, mixed methods research, as well as NLP, aim to derive insights from unstructured data. They also typically involve data in various formats like pdf documents, video and audio files, written transcripts, or even handwritten notes. Ingesting various file formats and integrating qualitative and quantitative data into a research process is challenging, time-consuming and potentially expensive. Integrating LLMs into these processes reduces technical challenges and speeds up processes. This in turn can create additional time to build more complex data pipelines, ingest even more diverse data, tackle larger data sets or engage in more complex data manipulations.  

Incorporating LLMs into NLP scripts like the OpenAI API allows for new approaches in qualitative data analysis, e.g. for qualitative coding and summarizing large numbers of documents such as interview transcripts. While this will make double-checking the results of GPT transformations a crucial part of the research process, researchers also gain time for the interpretation of the data and critical thinking. E.g. a lot of time is won by not having to train a machine learning model on how to process interview transcripts. GPT can perform tasks such as summarizing, qualitative coding, and entity or topic recognition on any text when given appropriate context. It can even generate tables or code to further process extracted entities within an IDE. Using Python libraries such as LangChain enables looping GPT over data frames or parsing outputs of prompts.   

## From Niche to Mainstream

Qualitative data analysis has traditionally been a niche method known predominantly among anthropologists. At the heart of the method stands qualitative coding, a process during which segments of e.g. text data, video or audio timelines are categorized and labeled. A code can be a simple text highlight or the assignment of a label. Within a project, codes can be put into relation to one another. They can also be [statistically analyzed](https://www.maxqda.com/products/maxqda-analytics-pro). Qualitative data analysis today takes place in sophisticated tools such as [MAXQDA](https://www.maxqda.com/). It's [four split screen](https://www.maxqda.com/help-mx22/screens-and-menus/the-maxqda-interface-and-the-four-main-windows) strongly resembles the IDEs used in computer and data science. There is an obvious potential for these methods to move closer together or to be used in conjunction with each other. In the flow chart, I have incorporated qualitative data analysis in the literature review as well as the analysis of collected data.

Understanding how to ingest and analyze different file formats, from qualitative and quantitative points of view, will become ever more important in the near future. This is because information today is no longer passed on primarily through books or newspapers: E.g. news pieces are increasingly generated through social media, knowledge is passed on via video tutorials and discussions take place in podcasts. Consequently, schoolchildren, today must learn how to gather and present information from the web from an early age. This increases the need for them to learn how to critically assess this information, relate it, and analyze it. 

In my experience, one of the key hurdles in doing this well is the idea of a linear process. Analyzing data from multiple sources in different formats can simply not be done in a step-by-step way. Going back and forth between different steps, revisiting and refining them based on gained insights, is essential. It helps to define a research question and identify the data needed to answer it. Concepts such as "iteration" or "agile" are commonly used in fields such as computer science and qualitative research. However, these principles are not necessarily well-understood outside of these circles. While "iteration" refers to the practice of continuously refining and improving, "agile" speaks to the ability to continuously adapt. Approaching data analysis in a linear way potentially leads to an abundance of information and difficulties in making meaningful connections between them. This may result in a feeling of overwhelm also known as cognitive overload. A lack of structure in the data can quickly lead to system overload and unfinished data pipelines. Therefore, structuring data analysis processes iteratively reduce its complexity. 

Looking ahead, I anticipate an increase in LLM-generated content. This will only increase the need for humans to critically assess it. In addition, LLMs are breaking technical barriers for potential new content creators e.g. when supporting the setup of editing tools. As a result, I am expecting more diverse voices to be heard shortly. This again will only increase the need for humans to critically analyze content in various forms, formats, and tones.
